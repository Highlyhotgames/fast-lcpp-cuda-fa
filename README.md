# fast-lcpp-cuda-fa
Build llama-cpp from source with CUDA in WSL (W11) and Flash Attention
